
<h1 align="center">
  <span> ğŸ‡»ğŸ‡³ Cá»™ng Ä‘á»“ng LLMs Viá»‡t Nam - Vietnamese Language Models Community</span>
</h1>

<div align="center">
     <img width="auto" height="400px" src="./images/Vietnamese_LLMs.png"/>
</div>

## ğŸ’¡ Get help - [Q&A](https://github.com/TranNhiem/Vietnamese_LLMs/discussions) or [Discord ğŸ’¬](https://discord.gg/eH7eg4fT)

# Ná»™i Dung (Table of Contents)

- [Giá»›i thiá»‡u vá» Vietnamese_LLMs](#Giá»›i-thiá»‡u-dá»±-Ã¡n)
- [Má»¥c tiÃªu dá»± Ã¡n](#cÃ¡c-liÃªn-káº¿t-há»¯u-Ã­ch)
- [CÃ¡ch tiáº¿n hÃ nh dá»± Ã¡n](#cÃ¡ch-thá»­-nghiá»‡m-open-assistant)
- [Cáº¥u trÃºc cá»§a dá»± Ã¡n](#Cáº¥u-trÃºc-cá»§a-dá»±-Ã¡n)
- [Táº§m nhÃ¬n](#táº§m-nhÃ¬n)
- [Káº¿ Hoáº¡ch](#káº¿-hoáº¡ch)
- [LÃ m tháº¿ nÃ o báº¡n cÃ³ thá»ƒ giÃºp Ä‘á»¡](#lÃ m-tháº¿-nÃ o-báº¡n-cÃ³-thá»ƒ-giÃºp-Ä‘á»¡)

## Giá»›i thiá»‡u dá»± Ã¡n (Project Introduction):

ChÃ o báº¡n Ä‘áº¿n vá»›i dá»± Ã¡n Cá»™ng Ä‘á»“ng LLMs Viá»‡t Nam! Dá»± Ã¡n vá»›i má»¥c tiÃªu táº¡o ra bá»™ dá»¯ liá»‡u Vietnamese instruction vÃ   thá»±c hiá»‡n Supervised instruction fine-tuning trÃªn cÃ¡c Open-source mÃ´ hÃ¬nh ngÃ´n ngá»¯  Bloom, OpenLLaMA, GPT-J, MPT, Pythia vÃ  nhiá»u mÃ´ hÃ¬nh khÃ¡c.

+ [Dá»± Ã¡n Tá»•ng Quan] ()

## Má»¥c tiÃªu dá»± Ã¡n (Project Goal):

- XÃ¢y dá»±ng Bá»™ dá»¯ liá»‡u HÆ°á»›ng dáº«n tiáº¿ng Viá»‡t cháº¥t lÆ°á»£ng cao
- Huáº¥n luyá»‡n, Tinh chá»‰nh vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh NgÃ´n ngá»¯ tiáº¿ng Viá»‡t (Training, Finetuning, Evaluation)
- Thiáº¿t káº¿ á»¨ng dá»¥ng vá»›i Giao diá»‡n NgÆ°á»i dÃ¹ng tá»‘i Æ°u hiá»‡u suáº¥t

## CÃ¡c nhiá»‡m vá»¥ (Tasks):

1. XÃ¢y dá»±ng Bá»™ dá»¯ liá»‡u Tiáº¿ng Viá»‡t cho HÆ°á»›ng dáº«n (Instructions) (cháº¥t lÆ°á»£ng, phong phÃº vÃ  Ä‘a dáº¡ng):
   - Chuyá»ƒn Ä‘á»•i cÃ¡c bá»™ dá»¯ liá»‡u HÆ°á»›ng dáº«n Tiáº¿ng Anh sang Tiáº¿ng Viá»‡t.
   - Tá»•ng há»£p cÃ¡c nguá»“n dá»¯ liá»‡u Ä‘a dáº¡ng cÃ³ sáºµn:
     + Sá»­ dá»¥ng bá»™ dá»¯ liá»‡u HÆ°á»›ng dáº«n Tiáº¿ng Viá»‡t tá»« wikiHow, vÃ­ dá»¥: [human-instruction Vietnamese dataset](https://www.kaggle.com/datasets/paolop/human-instructions-vietnamese-wikihow?resource=download).
     + Sá»­ dá»¥ng cÃ¡c bá»™ dá»¯ liá»‡u tá»« lÄ©nh vá»±c BÃ¡o chÃ­, Y há»c, GiÃ¡o dá»¥c, v.v., vÃ­ dá»¥: bá»™ dá»¯ liá»‡u tá»« BÃ¡o Corpus ([news-corpus](https://github.com/binhvq/news-corpus)).
   - Táº¡o bá»• sung bá»™ dá»¯ liá»‡u tá»± há»c (self-instruct):
     + Sá»­ dá»¥ng bá»™ dá»¯ liá»‡u tá»± há»c nhÆ° Stanford Alpaca.
     + Táº¡o bá»™ dá»¯ liá»‡u dá»±a trÃªn cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nhÆ° GPT-3, GPT-3.5, GPT-4, PALM2, v.v.

2. Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh NgÃ´n ngá»¯ (Training, Finetuning, Evaluating, Testing LLM):
   - Tinh chá»‰nh (Finetuning) cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ mÃ£ nguá»“n má»Ÿ nhÆ° bloomz, OpenLLaMA, GPT-J pythia, v.v. trÃªn Bá»™ dá»¯ liá»‡u HÆ°á»›ng dáº«n Tiáº¿ng Viá»‡t.
     + Ãp dá»¥ng cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u hÃ³a (Compression Machine learning) nhÆ° [Quantization](https://github.com/IST-DASLab/gptq), [Sparsity & Quantization](https://github.com/Vahe1994/SpQR).
     + Sá»­ dá»¥ng ká»¹ thuáº­t tinh chá»‰nh hiá»‡u quáº£ nhÆ° [LoRA]() vÃ  [QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
     + Ãp dá»¥ng cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u hÃ³a Huáº¥n luyá»‡n vÃ  Tinh chá»‰nh nhÆ° [Deepspeed](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/), [Colossal AI](https://colossalai.org/).
  - Báº¡n cÃ³ thá»ƒ theo dÃµi chi tiáº¿t model Finetuning [káº¿t quáº£](https://wandb.ai/tranrick/Vietnamese_LLMs/reports/Vietnamese_LLMs---Vmlldzo0NjM4Nzg3)
  - ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh trÃªn cÃ¡c bÃ i kiá»ƒm tra (Benchmark) vÃ  cÃ¡c tÃ¬nh huá»‘ng thá»±c táº¿.
   - Kiá»ƒm thá»­ mÃ´ hÃ¬nh trÃªn nhiá»u cÃ¡ch sá»­ dá»¥ng khÃ¡c nhau.

3. Thiáº¿t káº¿ á»¨ng dá»¥ng:
   - Thiáº¿t káº¿ Giao diá»‡n NgÆ°á»i dÃ¹ng (UI) thÃ¢n thiá»‡n vÃ  dá»… sá»­ dá»¥ng.
   - Tá»‘i Æ°u hiá»‡u suáº¥t á»©ng dá»¥ng.

## Cáº¥u TrÃºc Cá»§a Dá»± Ãn (Project Structure)

DÆ°á»›i Ä‘Ã¢y lÃ  cáº¥u trÃºc cá»§a dá»± Ã¡n, mÃ´ táº£ cÃ¡c pháº§n quan trá»ng vÃ  chá»©c nÄƒng chÃ­nh cá»§a chÃºng:

### 1. Táº¡o vÃ  Dá»‹ch CÃ¡c Bá»™ Dá»¯ liá»‡u (Generate and Translate Dataset)

ThÆ° má»¥c `/Generate_and_Translate_Dataset` chá»©a cÃ¡c bá»™ dá»¯ liá»‡u vÃ  cÃ´ng cá»¥ liÃªn quan Ä‘áº¿n viá»‡c táº¡o vÃ  dá»‹ch cÃ¡c instruction dataset.

- Pháº§n Dá»‹ch (Translation Dataset)

  - `Using_OpenAI_Translate_API.py`: Sá»­ dá»¥ng OpenAI GPT-3.5 vÃ  GPT-4 Ä‘á»ƒ dá»‹ch cÃ¡c bá»™ dá»¯ liá»‡u. ÄÃ¢y lÃ  má»™t phÆ°Æ¡ng phÃ¡p cho káº¿t quáº£ tá»‘t.

  - `Using_NLLB_MetaAI_Translate.py`: Sá»­ dá»¥ng NLLB lÃ m mÃ´ hÃ¬nh cho viá»‡c dá»‹ch. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng 54B model Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng Ä‘á»‘i.

- Pháº§n Táº¡o Instruction Dataset

  - `Generation_instruction_OpenAI_api.py`: Sá»­ dá»¥ng Stanford Alpaca template Ä‘á»ƒ táº¡o cÃ¡c instruction dataset. Gá»“m hÆ¡n 175 instruction tasks Ä‘Æ°á»£c táº¡o bá»Ÿi con ngÆ°á»i.

  - Cáº­p Nháº­p Sá»›m trong TÆ°Æ¡ng Lai: Pháº§n nÃ y dá»± kiáº¿n sáº½ Ä‘Æ°á»£c cáº­p nháº­t vá»›i thÃ´ng tin vá» cÃ¡ch táº¡o thÃªm Instruction dataset tá»« cÃ¡c nguá»“n khÃ¡c.

### 2. Training & Fine-tune LLM Model

ThÆ° má»¥c `/LLMs` chá»©a cÃ¡c tá»‡p tin vÃ  cÃ´ng cá»¥ Ä‘á»ƒ training vÃ  fine-tune cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ (Language Models).

- Pháº§n Fine-tuning dá»±a trÃªn cÃ¡c Open-Source Based LLMs (BLOOMZ, Open-LLaMA, v.v.)

  - `Finetune_llm_LoRA.py`: Cung cáº¥p cÃ´ng cá»¥ Ä‘á»ƒ fine-tune cÃ¡c mÃ´ hÃ¬nh LLMs dá»±a trÃªn cÃ¡c mÃ£ nguá»“n má»Ÿ nhÆ° BLOOMZ, Open-LLaMA, v.v.

  - `Finetune_llm_QLoRA.py`: ÄÃ¢y lÃ  má»™t cÃ´ng cá»¥ khÃ¡c Ä‘á»ƒ fine-tune cÃ¡c mÃ´ hÃ¬nh LLMs dá»±a trÃªn cÃ¡c mÃ£ nguá»“n má»Ÿ.

### 3. Giao Diá»‡n Web (Web UI Interface)

ThÆ° má»¥c `/WebUI` chá»©a cÃ¡c tá»‡p tin vÃ  cÃ´ng cá»¥ liÃªn quan Ä‘áº¿n giao diá»‡n ngÆ°á»i dÃ¹ng qua Web.

- Hiá»‡n táº¡i, Ä‘á»ƒ nhanh chÃ³ng vÃ  thuáº­n tiá»‡n cho viá»‡c demo vÃ  kiá»ƒm thá»­, chÃºng tÃ´i sá»­ dá»¥ng Gradio Ä‘á»ƒ phÃ¡t triá»ƒn giao diá»‡n.

  - `assistant_gradio.py`: ÄÃ¢y lÃ  á»©ng dá»¥ng Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a trÃªn Gradio, cho phÃ©p tráº£i nghiá»‡m trá»±c quan vÃ  trÃ² chuyá»‡n vá»›i trá»£ lÃ½ thÃ´ng qua giao diá»‡n Web.

Hy vá»ng Vá»›i cáº¥u trÃºc nÃ y, dá»± Ã¡n cÃ³ thá»ƒ Ä‘Æ°á»£c quáº£n lÃ½ má»™t cÃ¡ch cá»¥ thá»ƒ vÃ  dá»… Ä‘Ã ng Ä‘á»ƒ cáº­p nháº­p [má»i ngÆ°á»i cÃ³ thá»ƒ gÃ³p Ã½ Ä‘á»ƒ cÃ³ má»™t cáº¥u trÃºc tá»‘t hÆ¡n]()

## Táº§m NhÃ¬n (Project Vision)

[Chi Tiáº¿t vá» Vision & Roadmap](https://docs.google.com/presentation/d/1qfIQoGMmarlZWzRa5lVQrMD67SmoVb7F6jr5NS0_Hx0/edit?usp=sharing)

XÃ¢y dá»±ng trá»£ lÃ½ thÃ´ng minh tiáº¿ng Viá»‡t cá»§a tÆ°Æ¡ng lai, vÆ°á»£t trá»™i vÃ  linh hoáº¡t hÆ¡n bao giá» háº¿t!

+ ChÃºng ta sáº½ táº¡o ra má»™t mÃ´ hÃ¬nh LLMs (Language Models) tiÃªn tiáº¿n cÃ³ kháº£ nÄƒng xá»­ lÃ½ tá»‘t cÃ¡c tÃ¡c vá»¥ tiáº¿ng Viá»‡t. Má»¥c tiÃªu cá»§a phÃ¡t triá»ƒn máº¡nh cÃ¡c LLMs  á»©ng dá»¥ng khÃ¡c trong GiÃ¡o dá»¥c, Y táº¿, TÃ i chÃ­nh vÃ  CÃ´ng nghiá»‡p.

+ Äiá»u Ä‘áº·c biá»‡t lÃ  chÃºng ta muá»‘n táº¡o ra mÃ´ hÃ¬nh trá»£ lÃ½ cÃ³ kháº£ nÄƒng tÆ°Æ¡ng tÃ¡c nÃ¢ng cao trÃªn tiáº¿ng Viá»‡t. HÆ¡n tháº¿ ná»¯a chÃºng ta cÅ©ng Ä‘áº·t má»¥c tiÃªu lÃ m cho mÃ´ hÃ¬nh nÃ y nhá» gá»n vÃ  hiá»‡u quáº£, Ä‘á»ƒ cÃ³ thá»ƒ cháº¡y trÃªn cÃ¡c ngÆ°á»i dÃ¹ng cÃ¡ nhÃ¢n mÃ¡y tÃ­nh vá»›i cÃ¡c GPUs tháº¿ há»‡ tháº¥p vá»›i Ã­t memory.

+ Dá»± Ã¡n nÃ y chÃºng ta mong muá»‘n nháº­n Ä‘Æ°á»£c sá»± Ä‘Ã³ng gÃ³p vÃ  há»— trá»£ cá»™ng Ä‘á»“ng. HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng má»™t trá»£ lÃ½ thÃ´ng minh nÃ³i riÃªng vÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯a thuáº§n tiáº¿ng Viá»‡t nÃ³i chung vÃ  gÃ³p pháº§n táº¡o ra nhá»¯ng Ä‘Ã³ng gÃ³p Ã½ nghÄ©a cho cá»™ng Ä‘á»“ng Viá»‡t Nam ğŸ‡»ğŸ‡³.

## Káº¿ Hoáº¡ch (Project plan)

<!--  [Cáº¥u trÃºc cá»§a dá»± Ã¡n](https://docs.google.com/presentation/d/1OdCTI1vMpftOMTOXXHEt2Ck5SBLSkPf_Zwedq7n3wec/edit?usp=sharing) -->

### BÆ°á»›c 1: Dá»‹ch táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n
- Má»¥c tiÃªu: Dá»‹ch cÃ¡c bá»™ dá»¯ liá»‡u chuáº©n vÃ  cháº¥t LÆ°á»£ng English based instructions dataset : [Alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json), [Dolly 15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst1), [Filtered_ShareGPT](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) others dataset.
- XÃ¢y dá»±ng há»‡ thá»‘ng, thá»‘ng kÃª hiá»ƒn thá»‹ cÃ¡c chá»§ Ä‘á» khÃ¡c nhau trong táº­p dá»¯ liá»‡u Ä‘Ã£ thu tháº­p. Má»¥c Ä‘Ã­ch lÃ  loáº¡i bá» dá»¯ liá»‡u chá»©a thÃ´ng tin gÃ¢y láº·n, Ä‘á»™c háº¡i, spam, rÃ¡c rÆ°á»Ÿi hoáº·c thÃ´ng tin cÃ¡ nhÃ¢n hoáº·c cÃ¡c dá»¯ khÃ´ng Ä‘áº¡t yÃªu cáº§u.

### BÆ°á»›c 2: Táº¡o táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n tá»± Ä‘á»™ng
- Sá»­ dá»¥ng OpenAI GPT-3.5, GPT-4 Ä‘á»ƒ táº¡o táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n.
- Má»¥c tiÃªu: Thu tháº­p 500.000 Ä‘áº¿n 1 triá»‡u máº«u hÆ°á»›ng dáº«n Ä‘áº§u vÃ o + pháº£n há»“i (Instructions, outputs)
- Äá»“ng thá»i, chÃºng tÃ´i thu tháº­p cÃ¡c hÆ°á»›ng dáº«n Ä‘Æ°á»£c táº¡o bá»Ÿi con ngÆ°á»i cÃ³ sáºµn báº±ng tiáº¿ng Viá»‡t.

### BÆ°á»›c 3: Kiá»ƒm Ä‘á»‹nh vÃ  tiá»n xá»­ lÃ½ táº­p dá»¯ liá»‡u
- Káº¿t há»£p táº­p dá»¯ liá»‡u tá»« BÆ°á»›c 1 vÃ  BÆ°á»›c 2.
- Tiá»n xá»­ lÃ½ táº­p dá»¯ liá»‡u Ä‘á»ƒ chuáº©n bá»‹ cho cÃ¡c bÆ°á»›c tiáº¿p theo.

### BÆ°á»›c 4: Tiáº¿n hÃ nh giai Ä‘oáº¡n SFT (Supervised instruction Finetuning)
- Dá»±a trÃªn táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n tiáº¿ng Viá»‡t, tiáº¿n hÃ nh giai Ä‘oáº¡n SFT Ä‘á»ƒ tinh chá»‰nh mÃ´ hÃ¬nh.

### BÆ°á»›c 5: Tiáº¿p tá»¥c huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i giai Ä‘oáº¡n RLHF (Reinforcement Learning from Human Feedback)
- Sau khi hoÃ n thÃ nh BÆ°á»›c 4, chÃºng ta cÃ³ thá»ƒ tiáº¿p tá»¥c huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i giai Ä‘oáº¡n RLHF dá»±a trÃªn táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n tá»« con ngÆ°á»i thuá»™c dá»± Ã¡n OpenAssistant cÃ´ng khai.

HÃ£y nhá»› ráº±ng cÃ¡c bÆ°á»›c nÃ y Ä‘áº¡i diá»‡n cho quy trÃ¬nh chung vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh vÃ  bá»• sung theo yÃªu cáº§u cá»¥ thá»ƒ cá»§a dá»± Ã¡n.

## LÃ m Tháº¿ NÃ o Báº¡n CÃ³ GiÃºp Äá»¡ (How You can HELP)

ChÃºng ta cÃ³ thá»ƒ cÃ¹ng nhau Ä‘Ã³ng gÃ³p tri thá»©c vÃ  cÃ´ng nghá»‡ cá»§a mÃ¬nh Ä‘á»ƒ mang láº¡i lá»£i Ã­ch cho cá»™ng Ä‘á»“ng Viá»‡t Nam.

1. báº¡n cÃ³ thá»ƒ cÃ¹ng xÃ¢y dá»±ng dá»± Ã¡n: 
HÃ£y xem hÆ°á»›ng dáº«n [ÄÃ³ng GÃ³p Cho Dá»± Ãn](contribute.md) Ä‘á»ƒ báº¯t Ä‘áº§u chung tay xÃ¢y dá»±ng dá»± Ã¡n nÃ y.

2. Báº¡n cÃ³ thá»ƒ há»• trá»£ vá» tÃ i nguyÃªn nhÆ° mÃ¡y chá»§ server hoáº·c cÃ¡c tÃ i nguyÃªn khÃ¡c.
  - Dá»± Ã¡n hiá»‡n ráº¥t cáº§n cÃ¡c nguá»“n tÃ i trá»£ tÃ i nguyÃªn GPUs Ä‘á»ƒ cÃ³ thá»ƒ tiáº¿n hÃ nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n (Pretraining) vÃ  quÃ¡ trÃ¬nh tinh chá»‰nh (Finetuning).
  - Náº¿u báº¡n CÃ³ thá»ƒ giÃºp dá»± Ã¡n káº¿t ná»‘i vá»›i cÃ¡c cÃ´ng ty tÆ° nhÃ¢n Ä‘á»ƒ mang dá»± Ã¡n nÃ y Ã¡p dá»¥ng rá»™ng rÃ£i.
  - Báº¡n cÃ³ thá»ƒ káº¿t ná»‘i trá»±c tiáº¿p vá»›i Tráº§n Nhiá»‡m [LinkedIn](https://www.linkedin.com/in/tran-nhiem-ab1851125/) [Facebook](https://www.facebook.com/jean.tran.336). Nhiá»‡m vá»›i  vá»¥ cho Láº­p káº¿ hoáº¡ch vÃ  LÃªn lá»‹ch, TÃ i liá»‡u vÃ  BÃ¡o cÃ¡o.
